{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "functional-celebration",
   "metadata": {},
   "source": [
    "# Robotic Exploration Investigation (with Camera, Distance Sensor and Enhanced Analysis)\n",
    "\n",
    "In this investigation you will configure the Jetbot to move forward and then spin in order to capture a series of images and distances from its environment. The distances will be used to generate a 2D polar grid of the readings. The images will be processed through a DepthNet Model and MobileNet model to derive corresponding depth maps and identify objects from the robot's environment. This set of data will help you to gather and analyze visual information from an environment where you may have limited or no visibility. \n",
    "\n",
    "## 1. Import Libraries\n",
    "\n",
    "In this section, we import the necessary libraries. Next, we'll initialize the sensors and devices required for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-register",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "import uuid\n",
    "import subprocess\n",
    "from smbus2 import SMBus\n",
    "from VL53L1X import VL53L1X\n",
    "import cv2\n",
    "import ipywidgets as widgets\n",
    "import traitlets\n",
    "from IPython.display import display\n",
    "from jetbot import Camera, Robot, bgr8_to_jpeg\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import csv\n",
    "import jetson_inference\n",
    "import jetson_utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-expert",
   "metadata": {},
   "source": [
    "## 2. Initialization\n",
    "\n",
    "In this section, we initialize the distance sensor, camera, robot, and directory to store the data from our investigation. The ``range_value`` variable controls whether the distance sensor is calibrated for short, medium, or long distance ranges and may be modified based on your environment. The ``framerate`` variable controls how many distance sensor readings and camera frames are captured per second while the robot is collecting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize VL53L1X distance sensor\n",
    "# Set I2C bus number\n",
    "bus_number = 0\n",
    "\n",
    "#Set distance sensor range value\n",
    "# 0 = Unchanged\n",
    "# 1 = Short Range\n",
    "# 2 = Medium Range\n",
    "# 3 = Long Range\n",
    "range_value = 3\n",
    "\n",
    "tof = VL53L1X(i2c_bus=bus_number)\n",
    "tof.open()\n",
    "\n",
    "tof.start_ranging(range_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-jordan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize camera and robot\n",
    "camera = Camera.instance(width=640, height=360)\n",
    "robot = Robot()\n",
    "\n",
    "framerate = 20\n",
    "\n",
    "# Path to the snapshots directory\n",
    "snapshots_dir = 'snapshots'\n",
    "# Check if the snapshots directory exists\n",
    "if os.path.exists(snapshots_dir):\n",
    "    # Remove the directory and all its contents\n",
    "    shutil.rmtree(snapshots_dir)\n",
    "# Recreate the snapshots directory\n",
    "os.makedirs(snapshots_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-unemployment",
   "metadata": {},
   "source": [
    "## 3. Raw Data Visualization\n",
    "\n",
    "Here, we set up the displays for the camera feed and distance sensor. Ensure that the camera and distance sensor are working and oriented to capture the robot's environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-heading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cameara and distance widgets\n",
    "image_widget = widgets.Image(format='jpeg', width=camera.width, height=camera.height)\n",
    "distance_widget = widgets.Label(value=f\"Distance: {tof.get_distance()} mm\")\n",
    "display(image_widget, distance_widget)\n",
    "\n",
    "# Update display function\n",
    "def display_distance():\n",
    "    while True:\n",
    "        distance_widget.value = f\"Distance: {tof.get_distance()} mm\"\n",
    "        # Sleep for a short duration before the next update\n",
    "        time.sleep(0.2)\n",
    "\n",
    "# Start the update function in a separate thread\n",
    "thread = threading.Thread(target=display_distance)\n",
    "thread.start()\n",
    "\n",
    "link = traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-leonard",
   "metadata": {},
   "source": [
    "## 4. Robot Movement Calibration\n",
    "\n",
    "Movement on the Jetbot is controlled by modifying a combination of motor power and movement time. The surface that the Jetbot drives on can have a large impact on how far and fast it actually moves. This section progams your robot to turn. \n",
    "\n",
    "Use this section to calibrate the necessary motor power and movement time for your robot's environment. Place your robot in the environment where you plan to capture images and run the code block below. The goal is for the robot to complete a 360-degree turn as slowly as possible (allowing more data to be captured). Adjust the ``robot_speed`` and ``time_to_360`` variables until the robot is calibrated for these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate robot speed and duration for 360 degree turn\n",
    "robot_speed = 0.09\n",
    "time_to_360 = 6.0\n",
    "\n",
    "robot.left(robot_speed)\n",
    "time.sleep(time_to_360)  \n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-indianapolis",
   "metadata": {},
   "source": [
    "## 5. Raw Data Collection\n",
    "\n",
    "This section directs the robot to drive forward and capture frames from the camera and distance sensor readings. The ``robot_speed`` and ``time_to_360`` variables set in the previous step are used here.  An additional ``time_forward`` variable here may be modified to control the duration that the robot drives forward before capturing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable to control how long the robot drives forward before collecting data\n",
    "time_forward = 0\n",
    "\n",
    "# Capture raw video frames and distances as robot spins\n",
    "video_frames = []\n",
    "distance_values = []\n",
    "image_uuids = []\n",
    "\n",
    "def capture_data(duration=time_to_360):\n",
    "    start_time = time.time()\n",
    "    next_capture_time = start_time + 1.0/framerate\n",
    "    \n",
    "    while time.time() - start_time < duration:\n",
    "        current_time = time.time()\n",
    "        if current_time >= next_capture_time:\n",
    "            frame = camera.value\n",
    "            \n",
    "            # Extracting distance from the widget's value, assuming the format \"Distance: XXXX mm\"\n",
    "            distance_str = distance_widget.value.split(\":\")[1].strip().split(\" \")[0]\n",
    "            distance = float(distance_str)\n",
    "\n",
    "            # Generate a UUID and save it to the list\n",
    "            frame_uuid = str(uuid.uuid1())\n",
    "            image_uuids.append(frame_uuid)\n",
    "\n",
    "            video_frames.append((frame_uuid, frame))\n",
    "            distance_values.append(distance)\n",
    "            \n",
    "            # Adjust next capture time\n",
    "            next_capture_time += 1.0/framerate\n",
    "        else:\n",
    "            # Sleep only the remaining time\n",
    "            time.sleep(next_capture_time - current_time)\n",
    "\n",
    "    # Save the buffered frames to disk after capturing\n",
    "    for frame_uuid, frame in video_frames:\n",
    "        file_path = os.path.join(snapshots_dir, frame_uuid + '.jpg')\n",
    "        cv2.imwrite(file_path, frame)\n",
    "        \n",
    "#Drive robot forward desired duration\n",
    "robot.forward(0.3)\n",
    "time.sleep(time_forward)\n",
    "robot.stop()\n",
    "        \n",
    "#Turn robot and capture data\n",
    "robot.left(speed=robot_speed)\n",
    "capture_data()\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-yemen",
   "metadata": {},
   "source": [
    "## 6. Raw Data Display\n",
    "\n",
    "This section sets up a display where the camera frames and corresponding distance values may be inspected. Use the scrubber to inspect the data collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display raw video frames and distance values\n",
    "def display_playback(video_frames, distance_values):\n",
    "    # Create image and text widgets\n",
    "    image_widget = widgets.Image(format='jpeg', width=camera.width, height=camera.height)\n",
    "    distance_widget = widgets.Label(value=f\"Distance: {distance_values[0]} mm\")\n",
    "    slider = widgets.IntSlider(value=0, min=0, max=len(video_frames)-1, step=1, description=\"Frame\")\n",
    "    \n",
    "    # Update function for the widgets\n",
    "    def update_widgets(change):\n",
    "        image_widget.value = bgr8_to_jpeg(video_frames[slider.value][1])  # Access the frame from the tuple\n",
    "        distance_widget.value = f\"Distance: {distance_values[slider.value]} mm\"\n",
    "    \n",
    "    # Observe the slider changes\n",
    "    slider.observe(update_widgets, names='value')\n",
    "    \n",
    "    # Initial widget setup\n",
    "    image_widget.value = bgr8_to_jpeg(video_frames[0][1])  # Access the frame from the tuple\n",
    "    display(image_widget, slider, distance_widget)\n",
    "\n",
    "# Use the function to display captured data\n",
    "display_playback(video_frames, distance_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-control",
   "metadata": {},
   "source": [
    "## 7. Camera and Distance Sensor Cleanup\n",
    "\n",
    "This section stops the camera and distance sensor from continuing to run, freeing up resources for the remainder of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-moses",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop camera and distance sensor\n",
    "camera.stop()\n",
    "tof.stop_ranging()\n",
    "tof.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-instrument",
   "metadata": {},
   "source": [
    "## 8. Save Raw Video\n",
    "\n",
    "This step generates a video file of the raw frames from the camera feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-oregon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw video data\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('raw_video.avi', fourcc, framerate, (camera.width, camera.height))\n",
    "\n",
    "for frame_tuple in video_frames:\n",
    "    actual_frame = frame_tuple[1]  # Extract the frame from the tuple\n",
    "    out.write(actual_frame)\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-hotel",
   "metadata": {},
   "source": [
    "## 9. Function Definition for 2D Polar Grid of Distance Readings\n",
    "\n",
    "This section defines a function which generates a 2D polar grid of the distance sensor readings taken by the robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-sword",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definition to display distance sensor readings in a 2D polar grid\n",
    "def display_distance_values_polar_view(distance_values, max_distance=8000):\n",
    "    # Convert distance_values to numpy array if it's not\n",
    "    distance_values = np.array(distance_values)\n",
    "    \n",
    "    # Clip values and identify which are clipped\n",
    "    clipped = (distance_values > max_distance) | (distance_values < 0)\n",
    "    distance_values_clipped = np.minimum(distance_values, max_distance)\n",
    "    \n",
    "    # Generate theta values (rotate by 90 degrees, or pi/2 radians)\n",
    "    theta = np.linspace(np.pi/2, 2*np.pi + np.pi/2, len(distance_values))\n",
    "    \n",
    "    # Convert distance readings to Cartesian coordinates\n",
    "    x = distance_values_clipped * np.cos(theta)\n",
    "    y = distance_values_clipped * np.sin(theta)\n",
    "    \n",
    "    # Create a scatter plot with polar grid\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    ax.scatter(x[~clipped], y[~clipped], s=8, label='Valid Readings')  # Valid readings\n",
    "    ax.scatter(x[clipped], y[clipped], s=8, color='red', label='Clipped Readings')  # Clipped readings\n",
    "    \n",
    "    # Setting the limits\n",
    "    ax.set_xlim(-max_distance, max_distance)\n",
    "    ax.set_ylim(-max_distance, max_distance)\n",
    "    \n",
    "    # Remove the x and y axis labels and ticks\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    \n",
    "    ax.axhline(0, color='black',linewidth=0.5)\n",
    "    ax.axvline(0, color='black',linewidth=0.5)\n",
    "    \n",
    "    # Adding polar grid\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    for angle in np.linspace(0, 2*np.pi, 12, endpoint=False):  # 12 lines for 30 degree intervals\n",
    "        x_line = max_distance * np.cos(angle)\n",
    "        y_line = max_distance * np.sin(angle)\n",
    "        ax.plot([0, x_line], [0, y_line], color='gray', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    # Adding circular grid lines and their labels\n",
    "    for r in np.linspace(0, max_distance, 5):  # 5 circles as grid lines\n",
    "        circle = plt.Circle((0, 0), r, color='gray', fill=False, linestyle='--', linewidth=0.5)\n",
    "        ax.add_artist(circle)\n",
    "        # Add radial distance labels along one of the polar lines (choosing angle = pi/4 for placement)\n",
    "        ax.text(r * np.cos(np.pi/4), r * np.sin(np.pi/4), str(int(r)), color='gray', ha='center', va='center')\n",
    "    \n",
    "    # Add degree markers for 0°, 90°, 180°, and 270°.\n",
    "    ax.text(max_distance*0.01, max_distance*1.05, '0°', horizontalalignment='center')\n",
    "    ax.text(max_distance*1.05, 0, '270°', verticalalignment='center')\n",
    "    ax.text(-max_distance*0.01, -max_distance*1.1, '180°', horizontalalignment='center')\n",
    "    ax.text(-max_distance*1.1, 0, '90°', verticalalignment='center')\n",
    "    \n",
    "    # Adding arrow for initial orientation (0° heading)\n",
    "    arrow_length = max_distance * 0.0  # Length of the arrow for visibility\n",
    "    ax.arrow(0, 0, 0, arrow_length, head_width=max_distance*0.05, head_length=max_distance*0.1, fc='green', ec='green')\n",
    "    \n",
    "    # Adding legend\n",
    "    ax.legend()\n",
    "    \n",
    "    if os.path.exists('2D_plot.png'):\n",
    "        os.remove('2D_plot.png')\n",
    "    plt.savefig('2D_plot.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-brooks",
   "metadata": {},
   "source": [
    "## 10. Display 2D Polar Grid of Distance Readings\n",
    "\n",
    "This section runs the function, displaying the 2D polar grid of distance sensor readings. A ``cutoff_threshold`` variable may be set to \"clip\" distance readings that are outside of a useable range. Distance values above the ``cutoff_threshold`` variable will be set equal to its value and displayed as a red dot on the polar grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-partnership",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_threshold = 800\n",
    "\n",
    "# Plotting the distance values\n",
    "display_distance_values_polar_view(distance_values, cutoff_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-thread",
   "metadata": {},
   "source": [
    "## 11. Image Analysis: Panorama\n",
    "\n",
    "Now, let's stitch together a sample the raw camera frames that were taken into a panorama. An ``n`` variable allows you to specify the frequency of frrames used to generate the panorama.\n",
    "\n",
    "This process may take some time to complete depending on the number of frames used, and may fail if the algorithm is unable to determine the overlapping portions from one image to another. If the process succeeds, a panoramic image will be displayed below. If the process fails, you may need to adjust the frequency of frames used or the speed of the robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-clone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define value for 'n' to select every nth frame\n",
    "n = 10 \n",
    "\n",
    "# Select every nth frame tuple\n",
    "selected_frame_tuples = video_frames[::n]\n",
    "\n",
    "# Extract the actual frames from the selected tuples\n",
    "imgs = [frame_tuple[1] for frame_tuple in selected_frame_tuples]\n",
    "\n",
    "# Check if any image is empty or not read correctly\n",
    "for idx, img in enumerate(imgs):\n",
    "    if img is None:\n",
    "        print(f\"Image at index {idx} is empty or not read correctly. Please check the image path and format.\")\n",
    "\n",
    "# Stitch the images to create a panorama\n",
    "stitcher = cv2.Stitcher_create()\n",
    "(dummy, panorama) = stitcher.stitch(imgs)\n",
    "\n",
    "if dummy != cv2.STITCHER_OK:\n",
    "    print(\"Error in stitching images.\")\n",
    "else:\n",
    "    # Check if \"recent_panorama.jpg\" exists and remove it\n",
    "    if os.path.exists('camera_panorama.jpg'):\n",
    "        os.remove('camera_panorama.jpg')\n",
    "    \n",
    "    # Save the panorama as \"recent_panorama.jpg\"\n",
    "    cv2.imwrite('camera_panorama.jpg', panorama)\n",
    "\n",
    "    # Display the panorama using IPython's display functionality\n",
    "    from IPython.display import display, Image\n",
    "    import PIL.Image\n",
    "\n",
    "    # Convert OpenCV image to PIL image format for display\n",
    "    if panorama is not None:\n",
    "        panorama_pil = PIL.Image.fromarray(cv2.cvtColor(panorama, cv2.COLOR_BGR2RGB))\n",
    "        display(panorama_pil)\n",
    "    else:\n",
    "        print(\"The panorama is empty. It seems the images couldn't be stitched together.\")\n",
    "        \n",
    "del imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-weapon",
   "metadata": {},
   "source": [
    "## 12. Load the DepthNet Model\n",
    "\n",
    "DepthNet is designed to estimate depth from single RGB images. The \"fcn-resnet18\" variant in the notebook leverages a fully convolutional network (FCN) based on the ResNet-18 architecture. By processing RGB images, the model estimates depth information, which can be visualized as a depth map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DepthNet Model\n",
    "\n",
    "# Initialize the DepthNet model\n",
    "net_depth = jetson_inference.depthNet(\"fcn-resnet18\")\n",
    "depth_field = net_depth.GetDepthField()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-kennedy",
   "metadata": {},
   "source": [
    "## 13. Process Depth Maps\n",
    "\n",
    "In this step, the DepthNet model processes each raw video frame to generate a depth map with estimated minimum and maximum depth values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-receiver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Depth Maps\n",
    "depth_maps = []\n",
    "depth_min_values = []  # List to store min depth values for each frame\n",
    "depth_max_values = []  # List to store max depth values for each frame\n",
    "\n",
    "# Process each video frame through DepthNet\n",
    "def generate_depth_maps():\n",
    "    for idx, (uuid, frame) in enumerate(video_frames):  # Unpack the tuple here\n",
    "        cuda_img = jetson_utils.cudaFromNumpy(frame)\n",
    "        net_depth.Process(cuda_img)\n",
    "        depth_numpy = jetson_utils.cudaToNumpy(depth_field)\n",
    "    \n",
    "        # Check for NaN or Inf and replace them\n",
    "        depth_numpy = np.nan_to_num(depth_numpy)\n",
    "    \n",
    "        # Ensure we aren't dividing by zero in normalization\n",
    "        min_depth = np.min(depth_numpy)\n",
    "        max_depth = np.max(depth_numpy)\n",
    "        denom = max_depth - min_depth\n",
    "        if denom == 0:\n",
    "            denom = 1e-10  # Small constant to avoid division by zero\n",
    "    \n",
    "        # Normalize the depth values between 0 and 1\n",
    "        normalized_depth = (depth_numpy - min_depth) / denom\n",
    "    \n",
    "        # Scale the normalized values between 0 and 255\n",
    "        scaled_depth = (normalized_depth * 255).astype(np.uint8)\n",
    "    \n",
    "        # Apply the colormap\n",
    "        depth_colormap = cv2.applyColorMap(scaled_depth, cv2.COLORMAP_JET)\n",
    "    \n",
    "        # Resize to match the original frames\n",
    "        resized_depth_map = cv2.resize(depth_colormap, (camera.width, camera.height))\n",
    "    \n",
    "        # Use the UUID from the list to save the depth map\n",
    "        depth_file_path = os.path.join(snapshots_dir, uuid + '_depthMap.jpg')\n",
    "        cv2.imwrite(depth_file_path, resized_depth_map)\n",
    "    \n",
    "        # Append the min and max values to their respective lists\n",
    "        depth_min_values.append(min_depth)\n",
    "        depth_max_values.append(max_depth)\n",
    "    \n",
    "        depth_maps.append(resized_depth_map)\n",
    "\n",
    "# Calling the function (for demonstration purposes)\n",
    "generate_depth_maps()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-coffee",
   "metadata": {},
   "source": [
    "## 14. Display Depth Maps\n",
    "\n",
    "This section sets up a display where the raw camera frames and corresponding depth maps may be inspected. Use the scrubber to inspect the data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Depth Maps and Depth Estimates side-by-side with original frames\n",
    "\n",
    "def display_frames_with_depthMaps(video_frames, depth_maps, depth_min_values, depth_max_values):\n",
    "    # Create image widgets\n",
    "    orig_image_widget = widgets.Image(format='jpeg', width=camera.width, height=camera.height)\n",
    "    depth_image_widget = widgets.Image(format='jpeg', width=camera.width, height=camera.height)\n",
    "    slider = widgets.IntSlider(value=0, min=0, max=len(video_frames)-1, step=1, description=\"Frame\")\n",
    "    \n",
    "    # Create labels for min and max distances\n",
    "    min_depth_label = widgets.Label(value=f\"Min Depth: {depth_min_values[0]:.2f} units\")\n",
    "    max_depth_label = widgets.Label(value=f\"Max Depth: {depth_max_values[0]:.2f} units\")\n",
    "    \n",
    "    # Group min and max depth labels in an HBox\n",
    "    depth_labels_hbox = widgets.HBox([min_depth_label, max_depth_label])\n",
    "    \n",
    "    # Group the depth image widget and the depth labels in a VBox\n",
    "    depth_vbox = widgets.VBox([depth_image_widget, depth_labels_hbox])\n",
    "    image_vbox = widgets.VBox([orig_image_widget, slider])\n",
    "    \n",
    "    # Update function for the widgets\n",
    "    def update_widgets(change):\n",
    "        orig_image_widget.value = bgr8_to_jpeg(video_frames[slider.value][1])  # Extract the frame from the tuple\n",
    "        depth_image_widget.value = cv2.imencode('.jpg', depth_maps[slider.value])[1].tobytes()\n",
    "        \n",
    "        # Update min and max depth labels\n",
    "        min_depth_label.value = f\"Min Depth: {depth_min_values[slider.value]:.2f} units\"\n",
    "        max_depth_label.value = f\"Max Depth: {depth_max_values[slider.value]:.2f} units\"\n",
    "    \n",
    "    # Observe the slider changes\n",
    "    slider.observe(update_widgets, names='value')\n",
    "    \n",
    "    # Initial widget setup\n",
    "    orig_image_widget.value = bgr8_to_jpeg(video_frames[0][1])  # Extract the frame from the tuple\n",
    "    depth_image_widget.value = cv2.imencode('.jpg', depth_maps[0])[1].tobytes()\n",
    "    \n",
    "    # Display the widgets using an outer HBox\n",
    "    display(widgets.HBox([image_vbox, depth_vbox]))\n",
    "\n",
    "# Call the modified function to display the frames\n",
    "display_frames_with_depthMaps(video_frames, depth_maps, depth_min_values, depth_max_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-emission",
   "metadata": {},
   "source": [
    "## 15. Load the MobileNet Model\n",
    "\n",
    "The SSD-MobileNet-v2 model performs object recognition. The ``threshold`` parameter specifies the confidence level below which detections will be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MobileNet model\n",
    "net_mobile = jetson_inference.detectNet(\"ssd-mobilenet-v2\", threshold=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-pharmacology",
   "metadata": {},
   "source": [
    "## 16. Perform Object Recognition\n",
    "\n",
    "In this step, the MobileNet model processes each raw video frame to identify objects above the necessary confidence threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process frames for object detection\n",
    "object_out = cv2.VideoWriter('object_detection_video.avi', cv2.VideoWriter_fourcc(*'XVID'), framerate, (camera.width, camera.height))\n",
    "\n",
    "# Create image widget\n",
    "object_detection_widget = widgets.Image(format='jpeg', width=camera.width, height=camera.height)\n",
    "\n",
    "# Create a list to store the detection results for each frame\n",
    "detection_results = []\n",
    "\n",
    "# Enhanced object detection loop to store object names and their confidence scores\n",
    "for (uuid, frame) in video_frames:  # Unpack the tuple here\n",
    "    cuda_img = jetson_utils.cudaFromNumpy(frame)\n",
    "    detections = net_mobile.Detect(cuda_img, overlay=\"box,labels,conf\")\n",
    "    \n",
    "    # Storing detected object names and their confidence scores for the current frame\n",
    "    frame_detections = [(det.ClassID, net_mobile.GetClassDesc(det.ClassID), det.Confidence) for det in detections]\n",
    "    detection_results.append(frame_detections)\n",
    "    \n",
    "    processed_img = jetson_utils.cudaToNumpy(cuda_img)\n",
    "    object_detection_widget.value = cv2.imencode('.jpg', processed_img)[1].tobytes()\n",
    "    object_out.write(processed_img)\n",
    "object_out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-syndrome",
   "metadata": {},
   "source": [
    "## 17. Display Object Recognition\n",
    "\n",
    "This section sets up a display where the camera frames overlayed with objects recognized with the MobileNet model may be inspected. Use the scrubber to inspect the data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-campbell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the frames with the object detection\n",
    "def display_object_detections(video_frames):\n",
    "    # Create a slider for frame navigation\n",
    "    slider = widgets.IntSlider(value=0, min=0, max=len(video_frames)-1, step=1, description=\"Frame\")\n",
    "    \n",
    "    # Create a list to store processed frames (with object detections)\n",
    "    processed_frames = []\n",
    "    \n",
    "    # Process video and save object detections\n",
    "    for (uuid, frame) in video_frames:  # Unpack the tuple here\n",
    "        cuda_img = jetson_utils.cudaFromNumpy(frame)\n",
    "        detections = net_mobile.Detect(cuda_img, overlay=\"box,labels,conf\")\n",
    "        processed_img = jetson_utils.cudaToNumpy(cuda_img)\n",
    "        processed_frames.append(processed_img)\n",
    "\n",
    "    # Update function for the widget\n",
    "    def update_frame(change):\n",
    "        object_detection_widget.value = cv2.imencode('.jpg', processed_frames[slider.value])[1].tobytes()\n",
    "\n",
    "    # Attach the update function to the slider\n",
    "    slider.observe(update_frame, names='value')\n",
    "    \n",
    "    # Initial display setup\n",
    "    object_detection_widget.value = cv2.imencode('.jpg', processed_frames[0])[1].tobytes()\n",
    "    \n",
    "    # Display the widget and slider\n",
    "    display(object_detection_widget, slider)\n",
    "    \n",
    "display_object_detections(video_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-wyoming",
   "metadata": {},
   "source": [
    "## 18. Generate Data Log\n",
    "\n",
    "This section generates a CSV log file containing references to all raw camera frames, their corresponding distance sensor reading, and all corresponding data generated through the DepthNet and MobileNet analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-secondary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate CSV log of all data captured and analyzed\n",
    "log_file_path = 'data_log.csv'\n",
    "with open(log_file_path, 'w', newline='') as csvfile:\n",
    "    log_writer = csv.writer(csvfile)\n",
    "    \n",
    "    # Write headers\n",
    "    log_writer.writerow([\"File Name\", \"Distance\", \"Depth Map Name\", \"Min Depth Value\", \"Max Depth Value\", \"Detected Objects and Confidence\"])\n",
    "    \n",
    "    # Write data for each frame\n",
    "    for idx, image_uuid in enumerate(image_uuids):\n",
    "        # Creating a string representation for detected objects and their confidence scores\n",
    "        objects_str = \"; \".join([f\"{det[1]} ({det[2]:.2f})\" for det in detection_results[idx]])\n",
    "\n",
    "        log_writer.writerow([\n",
    "            f\"{image_uuid}.jpg\",\n",
    "            distance_values[idx],\n",
    "            f\"{image_uuid}_depthMap.jpg\",\n",
    "            depth_min_values[idx],\n",
    "            depth_max_values[idx],\n",
    "            objects_str\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-compiler",
   "metadata": {},
   "source": [
    "## 19. Generate Data Archive\n",
    "\n",
    "This section generates a ZIP file containing all captured and processed data generated through the investigation. It may be downloaded for further dissemination and archival. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-warning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to zip the generated data\n",
    "def zip_generated_files():\n",
    "    # Specify the files and directories to be zipped\n",
    "    files_to_zip = [\n",
    "        'data_log.csv',  # The log file\n",
    "        'raw_video.avi',\n",
    "        'object_detection_video.avi',\n",
    "        'camera_panorama.jpg',\n",
    "        '2D_plot.png',\n",
    "        snapshots_dir  # Directory containing original images and depth maps\n",
    "        # Add other files like panorama, 2D plot, regular video, etc. if they are generated in the notebook\n",
    "    ]\n",
    "    \n",
    "    # Create the zip file\n",
    "    zip_filename = \"data_analysis_files.zip\"\n",
    "    with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "        for file_or_dir in files_to_zip:\n",
    "            if os.path.isfile(file_or_dir):\n",
    "                # Add individual file to the zip\n",
    "                zipf.write(file_or_dir)\n",
    "            elif os.path.isdir(file_or_dir):\n",
    "                # Add directory and all its contents to the zip\n",
    "                for foldername, subfolders, filenames in os.walk(file_or_dir):\n",
    "                    for filename in filenames:\n",
    "                        file_path = os.path.join(foldername, filename)\n",
    "                        zipf.write(file_path, os.path.relpath(file_path, file_or_dir))\n",
    "    \n",
    "    return zip_filename  # Return the path to the generated zip file for verification\n",
    "\n",
    "# Call the function to create the zip file\n",
    "zip_generated_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-hollow",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this investigation, you successfully captured raw camera and distance sensor data from the robot's environment. You then performed a thorough analysis on the data, allowing you to gain insight into the robot's surroundings. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
